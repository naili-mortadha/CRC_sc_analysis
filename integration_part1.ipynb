{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c1151d-bc05-4bd0-a53d-d021e2987743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28e4623-a1bd-4921-89de-12f9ad894cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ecd5f5-2e90-41b2-afd0-f427d29fe199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scvi.autotune import ModelTuner\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d5f33d-9b64-404a-96b5-f1a32dd51845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1.post1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scvi.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439dde41-30c6-42ad-9a39-ff054d23196d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 58015 × 18505\n",
       "    obs: 'samples', 'condition', 'location', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'pct_counts_mt', 'pct_counts_ribo', 'pct_counts_hb', 'low_label', 'low_score'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = sc.read('unintigrated.h5ad')\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d84c18-6344-4626-a377-12f1b413aec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.pp.highly_variable_genes(adata, n_top_genes = 3000, subset = True, flavor = 'seurat_v3')\n",
    "#adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c70eed6-c252-4a6e-a758-53d6f1dcd08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 58015 × 14469\n",
       "    obs: 'samples', 'condition', 'location', 'n_genes', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_20_genes', 'pct_counts_mt', 'pct_counts_ribo', 'pct_counts_hb', 'low_label', 'low_score'\n",
       "    var: 'n_cells'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.pp.filter_genes(adata, min_cells = 100)\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be0d224f-3143-4209-ae86-13ae1846df9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "model_cls = scvi.model.SCVI\n",
    "model_cls.setup_anndata(adata, categorical_covariate_keys = ['samples'],\n",
    "                             continuous_covariate_keys=['pct_counts_mt', 'pct_counts_ribo'])\n",
    "\n",
    "tuner = ModelTuner(model_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0e8e878-2f54-416d-bb74-f1bbcefa2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"n_hidden\": tune.choice([92, 128, 192, 256]),\n",
    "    \"n_latent\": tune.choice([10, 20, 30, 40, 50, 60]),\n",
    "    \"n_layers\": tune.choice([1, 2, 3]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "    \"gene_likelihood\": tune.choice([\"nb\", \"zinb\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1482d589-994c-4365-a991-2c2870772103",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-07-15 16:45:33</td></tr>\n",
       "<tr><td>Running for: </td><td>00:58:23.39        </td></tr>\n",
       "<tr><td>Memory:      </td><td>11.1/62.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=59<br>Bracket: Iter 64.000: None | Iter 32.000: None | Iter 16.000: -3484.387939453125 | Iter 8.000: -3506.39111328125 | Iter 4.000: -3539.138427734375 | Iter 2.000: -3625.69873046875 | Iter 1.000: -3759.068603515625<br>Logical resource usage: 0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  n_hidden</th><th style=\"text-align: right;\">  n_latent</th><th style=\"text-align: right;\">  n_layers</th><th style=\"text-align: right;\">         lr</th><th>gene_likelihood  </th><th style=\"text-align: right;\">  validation_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>_trainable_f5fc6fcd</td><td>TERMINATED</td><td>192.168.1.136:42999</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">0.00210631 </td><td>zinb             </td><td style=\"text-align: right;\">          3496.34</td></tr>\n",
       "<tr><td>_trainable_e890cdae</td><td>TERMINATED</td><td>192.168.1.136:43842</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        50</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.000259943</td><td>nb               </td><td style=\"text-align: right;\">          4080.2 </td></tr>\n",
       "<tr><td>_trainable_e05e0fc2</td><td>TERMINATED</td><td>192.168.1.136:43974</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">0.000437623</td><td>zinb             </td><td style=\"text-align: right;\">          3923.32</td></tr>\n",
       "<tr><td>_trainable_cfb00b4d</td><td>TERMINATED</td><td>192.168.1.136:44127</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.00468494 </td><td>zinb             </td><td style=\"text-align: right;\">          3525.14</td></tr>\n",
       "<tr><td>_trainable_c4155897</td><td>TERMINATED</td><td>192.168.1.136:44454</td><td style=\"text-align: right;\">        92</td><td style=\"text-align: right;\">        50</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00648928 </td><td>zinb             </td><td style=\"text-align: right;\">          3524.23</td></tr>\n",
       "<tr><td>_trainable_d6d114ad</td><td>TERMINATED</td><td>192.168.1.136:44803</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        50</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">0.0021775  </td><td>nb               </td><td style=\"text-align: right;\">          3785.94</td></tr>\n",
       "<tr><td>_trainable_741f927e</td><td>TERMINATED</td><td>192.168.1.136:44933</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">0.00466232 </td><td>zinb             </td><td style=\"text-align: right;\">          3628.7 </td></tr>\n",
       "<tr><td>_trainable_daeedccf</td><td>TERMINATED</td><td>192.168.1.136:45079</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.000108571</td><td>nb               </td><td style=\"text-align: right;\">          4749.04</td></tr>\n",
       "<tr><td>_trainable_1baf5e6e</td><td>TERMINATED</td><td>192.168.1.136:45215</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">        30</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">0.00088155 </td><td>nb               </td><td style=\"text-align: right;\">          3866.64</td></tr>\n",
       "<tr><td>_trainable_a1be96f2</td><td>TERMINATED</td><td>192.168.1.136:45355</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        50</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">0.000320568</td><td>nb               </td><td style=\"text-align: right;\">          4121.67</td></tr>\n",
       "<tr><td>_trainable_eeae5d9b</td><td>TERMINATED</td><td>192.168.1.136:45481</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">0.00116245 </td><td>zinb             </td><td style=\"text-align: right;\">          3675.86</td></tr>\n",
       "<tr><td>_trainable_7590f47e</td><td>TERMINATED</td><td>192.168.1.136:45635</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        50</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.00387348 </td><td>nb               </td><td style=\"text-align: right;\">          3651.61</td></tr>\n",
       "<tr><td>_trainable_bfaefb6f</td><td>TERMINATED</td><td>192.168.1.136:45833</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.000915264</td><td>nb               </td><td style=\"text-align: right;\">          3906.01</td></tr>\n",
       "<tr><td>_trainable_dd190851</td><td>TERMINATED</td><td>192.168.1.136:45957</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">        30</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">0.000135236</td><td>zinb             </td><td style=\"text-align: right;\">          4025.36</td></tr>\n",
       "<tr><td>_trainable_dd37fcbe</td><td>TERMINATED</td><td>192.168.1.136:46114</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        60</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.000120209</td><td>nb               </td><td style=\"text-align: right;\">          4444.3 </td></tr>\n",
       "<tr><td>_trainable_7e083ab0</td><td>TERMINATED</td><td>192.168.1.136:46243</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00819392 </td><td>nb               </td><td style=\"text-align: right;\">          3504.89</td></tr>\n",
       "<tr><td>_trainable_593a1ce3</td><td>TERMINATED</td><td>192.168.1.136:47033</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">        50</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">0.00180157 </td><td>zinb             </td><td style=\"text-align: right;\">          3647.6 </td></tr>\n",
       "<tr><td>_trainable_422721fc</td><td>TERMINATED</td><td>192.168.1.136:47201</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.000230143</td><td>nb               </td><td style=\"text-align: right;\">          4067.46</td></tr>\n",
       "<tr><td>_trainable_857985c7</td><td>TERMINATED</td><td>192.168.1.136:47329</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        60</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">0.000971234</td><td>nb               </td><td style=\"text-align: right;\">          3904.96</td></tr>\n",
       "<tr><td>_trainable_8ee7ee53</td><td>TERMINATED</td><td>192.168.1.136:47480</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.000216182</td><td>zinb             </td><td style=\"text-align: right;\">          3924.74</td></tr>\n",
       "<tr><td>_trainable_83aadf24</td><td>TERMINATED</td><td>192.168.1.136:47614</td><td style=\"text-align: right;\">        92</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00959134 </td><td>zinb             </td><td style=\"text-align: right;\">          3481.23</td></tr>\n",
       "<tr><td>_trainable_95546113</td><td>TERMINATED</td><td>192.168.1.136:48678</td><td style=\"text-align: right;\">        92</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00252382 </td><td>zinb             </td><td style=\"text-align: right;\">          3669.92</td></tr>\n",
       "<tr><td>_trainable_3e8d38a9</td><td>TERMINATED</td><td>192.168.1.136:48952</td><td style=\"text-align: right;\">        92</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00903419 </td><td>zinb             </td><td style=\"text-align: right;\">          3586.04</td></tr>\n",
       "<tr><td>_trainable_d2c398b6</td><td>TERMINATED</td><td>192.168.1.136:49317</td><td style=\"text-align: right;\">        92</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.000589094</td><td>zinb             </td><td style=\"text-align: right;\">          3863.45</td></tr>\n",
       "<tr><td>_trainable_a00b9792</td><td>TERMINATED</td><td>192.168.1.136:49586</td><td style=\"text-align: right;\">        92</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00291304 </td><td>zinb             </td><td style=\"text-align: right;\">          3687.62</td></tr>\n",
       "<tr><td>_trainable_fc0b4a9b</td><td>TERMINATED</td><td>192.168.1.136:49870</td><td style=\"text-align: right;\">        92</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">0.00151835 </td><td>zinb             </td><td style=\"text-align: right;\">          3818.05</td></tr>\n",
       "<tr><td>_trainable_8ae2fdba</td><td>TERMINATED</td><td>192.168.1.136:50071</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.0069506  </td><td>zinb             </td><td style=\"text-align: right;\">          3488.29</td></tr>\n",
       "<tr><td>_trainable_4702dde5</td><td>TERMINATED</td><td>192.168.1.136:51360</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">0.00312893 </td><td>zinb             </td><td style=\"text-align: right;\">          3653.7 </td></tr>\n",
       "<tr><td>_trainable_80b7adb7</td><td>TERMINATED</td><td>192.168.1.136:51643</td><td style=\"text-align: right;\">        92</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.00614294 </td><td>zinb             </td><td style=\"text-align: right;\">          3500.3 </td></tr>\n",
       "<tr><td>_trainable_4a876b29</td><td>TERMINATED</td><td>192.168.1.136:53157</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        30</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.009208   </td><td>zinb             </td><td style=\"text-align: right;\">          3567.89</td></tr>\n",
       "<tr><td>_trainable_f6aaed6a</td><td>TERMINATED</td><td>192.168.1.136:53495</td><td style=\"text-align: right;\">        92</td><td style=\"text-align: right;\">        60</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.00538284 </td><td>zinb             </td><td style=\"text-align: right;\">          3547.99</td></tr>\n",
       "<tr><td>_trainable_a11fa97a</td><td>TERMINATED</td><td>192.168.1.136:53942</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.00736223 </td><td>zinb             </td><td style=\"text-align: right;\">          3565.21</td></tr>\n",
       "<tr><td>_trainable_7e986917</td><td>TERMINATED</td><td>192.168.1.136:54625</td><td style=\"text-align: right;\">        92</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.00373468 </td><td>zinb             </td><td style=\"text-align: right;\">          3639.96</td></tr>\n",
       "<tr><td>_trainable_6aacdf09</td><td>TERMINATED</td><td>192.168.1.136:54913</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.00146664 </td><td>zinb             </td><td style=\"text-align: right;\">          3798.94</td></tr>\n",
       "<tr><td>_trainable_a0d22b8f</td><td>TERMINATED</td><td>192.168.1.136:55094</td><td style=\"text-align: right;\">        92</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.000593361</td><td>zinb             </td><td style=\"text-align: right;\">          3940.08</td></tr>\n",
       "<tr><td>_trainable_4085a1d3</td><td>TERMINATED</td><td>192.168.1.136:55352</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.0046942  </td><td>zinb             </td><td style=\"text-align: right;\">          3493.37</td></tr>\n",
       "<tr><td>_trainable_d1df6d1b</td><td>TERMINATED</td><td>192.168.1.136:56119</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        30</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00661684 </td><td>zinb             </td><td style=\"text-align: right;\">          3627.39</td></tr>\n",
       "<tr><td>_trainable_26c0befb</td><td>TERMINATED</td><td>192.168.1.136:56290</td><td style=\"text-align: right;\">        92</td><td style=\"text-align: right;\">        60</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.0098838  </td><td>zinb             </td><td style=\"text-align: right;\">          3523.96</td></tr>\n",
       "<tr><td>_trainable_fd5eded7</td><td>TERMINATED</td><td>192.168.1.136:56719</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00367002 </td><td>zinb             </td><td style=\"text-align: right;\">          3627.88</td></tr>\n",
       "<tr><td>_trainable_07e48b39</td><td>TERMINATED</td><td>192.168.1.136:56887</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.00221524 </td><td>zinb             </td><td style=\"text-align: right;\">          3645.83</td></tr>\n",
       "<tr><td>_trainable_d13d1272</td><td>TERMINATED</td><td>192.168.1.136:57173</td><td style=\"text-align: right;\">        92</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00541033 </td><td>zinb             </td><td style=\"text-align: right;\">          3628.28</td></tr>\n",
       "<tr><td>_trainable_e048512e</td><td>TERMINATED</td><td>192.168.1.136:57329</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        50</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.000649605</td><td>zinb             </td><td style=\"text-align: right;\">          3819.05</td></tr>\n",
       "<tr><td>_trainable_303726ec</td><td>TERMINATED</td><td>192.168.1.136:57475</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        30</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00032055 </td><td>zinb             </td><td style=\"text-align: right;\">          4026.02</td></tr>\n",
       "<tr><td>_trainable_21c81ff5</td><td>TERMINATED</td><td>192.168.1.136:57661</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.00129563 </td><td>nb               </td><td style=\"text-align: right;\">          3811.92</td></tr>\n",
       "<tr><td>_trainable_4d30a4a8</td><td>TERMINATED</td><td>192.168.1.136:57792</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00981492 </td><td>zinb             </td><td style=\"text-align: right;\">          3466.3 </td></tr>\n",
       "<tr><td>_trainable_23d7c2b5</td><td>TERMINATED</td><td>192.168.1.136:58562</td><td style=\"text-align: right;\">        92</td><td style=\"text-align: right;\">        50</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">0.00195126 </td><td>nb               </td><td style=\"text-align: right;\">          3812.8 </td></tr>\n",
       "<tr><td>_trainable_6a8d729a</td><td>TERMINATED</td><td>192.168.1.136:58702</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.000168034</td><td>zinb             </td><td style=\"text-align: right;\">          4089.14</td></tr>\n",
       "<tr><td>_trainable_7668f5ed</td><td>TERMINATED</td><td>192.168.1.136:58864</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00440426 </td><td>nb               </td><td style=\"text-align: right;\">          3448.28</td></tr>\n",
       "<tr><td>_trainable_914efb61</td><td>TERMINATED</td><td>192.168.1.136:59410</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.000765036</td><td>zinb             </td><td style=\"text-align: right;\">          3763.78</td></tr>\n",
       "<tr><td>_trainable_bbb3b358</td><td>TERMINATED</td><td>192.168.1.136:59620</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00462097 </td><td>nb               </td><td style=\"text-align: right;\">          3647.54</td></tr>\n",
       "<tr><td>_trainable_0f1d4e85</td><td>TERMINATED</td><td>192.168.1.136:59785</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.000446458</td><td>nb               </td><td style=\"text-align: right;\">          3963.38</td></tr>\n",
       "<tr><td>_trainable_f3933985</td><td>TERMINATED</td><td>192.168.1.136:59978</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00306163 </td><td>nb               </td><td style=\"text-align: right;\">          3764.78</td></tr>\n",
       "<tr><td>_trainable_5a768c67</td><td>TERMINATED</td><td>192.168.1.136:60206</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00255893 </td><td>nb               </td><td style=\"text-align: right;\">          3643.25</td></tr>\n",
       "<tr><td>_trainable_f2a0617a</td><td>TERMINATED</td><td>192.168.1.136:60346</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00795953 </td><td>nb               </td><td style=\"text-align: right;\">          3486.08</td></tr>\n",
       "<tr><td>_trainable_06d3739c</td><td>TERMINATED</td><td>192.168.1.136:60937</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">0.00436929 </td><td>nb               </td><td style=\"text-align: right;\">          3762.02</td></tr>\n",
       "<tr><td>_trainable_8c5de863</td><td>TERMINATED</td><td>192.168.1.136:61064</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00162385 </td><td>nb               </td><td style=\"text-align: right;\">          3791.14</td></tr>\n",
       "<tr><td>_trainable_682b0c52</td><td>TERMINATED</td><td>192.168.1.136:61182</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00569926 </td><td>nb               </td><td style=\"text-align: right;\">          3681.3 </td></tr>\n",
       "<tr><td>_trainable_8250e27c</td><td>TERMINATED</td><td>192.168.1.136:61336</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        60</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">0.00121327 </td><td>nb               </td><td style=\"text-align: right;\">          3788.2 </td></tr>\n",
       "<tr><td>_trainable_14d0fd73</td><td>TERMINATED</td><td>192.168.1.136:61465</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00046369 </td><td>nb               </td><td style=\"text-align: right;\">          3951.51</td></tr>\n",
       "<tr><td>_trainable_a014ff4d</td><td>TERMINATED</td><td>192.168.1.136:61595</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">        50</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00107389 </td><td>nb               </td><td style=\"text-align: right;\">          3681.34</td></tr>\n",
       "<tr><td>_trainable_600bdca6</td><td>TERMINATED</td><td>192.168.1.136:61733</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        30</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">0.00251965 </td><td>nb               </td><td style=\"text-align: right;\">          3799.26</td></tr>\n",
       "<tr><td>_trainable_833a08ec</td><td>TERMINATED</td><td>192.168.1.136:61864</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.0082919  </td><td>nb               </td><td style=\"text-align: right;\">          3480.34</td></tr>\n",
       "<tr><td>_trainable_ec41e01a</td><td>TERMINATED</td><td>192.168.1.136:62323</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">        60</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00409888 </td><td>nb               </td><td style=\"text-align: right;\">          3455.99</td></tr>\n",
       "<tr><td>_trainable_e6181c3c</td><td>TERMINATED</td><td>192.168.1.136:62820</td><td style=\"text-align: right;\">       192</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00355031 </td><td>nb               </td><td style=\"text-align: right;\">          3770.27</td></tr>\n",
       "<tr><td>_trainable_281c09d5</td><td>TERMINATED</td><td>192.168.1.136:62957</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">        60</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">0.000780405</td><td>nb               </td><td style=\"text-align: right;\">          3851.57</td></tr>\n",
       "<tr><td>_trainable_5fbf9ed4</td><td>TERMINATED</td><td>192.168.1.136:63084</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">        60</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00418913 </td><td>nb               </td><td style=\"text-align: right;\">          3643.47</td></tr>\n",
       "<tr><td>_trainable_f008ca30</td><td>TERMINATED</td><td>192.168.1.136:63228</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">        60</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00519798 </td><td>nb               </td><td style=\"text-align: right;\">          3450.66</td></tr>\n",
       "<tr><td>_trainable_434476a6</td><td>TERMINATED</td><td>192.168.1.136:63747</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">        60</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00650174 </td><td>nb               </td><td style=\"text-align: right;\">          3438.97</td></tr>\n",
       "<tr><td>_trainable_19c6afbc</td><td>TERMINATED</td><td>192.168.1.136:64377</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">        60</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00214864 </td><td>nb               </td><td style=\"text-align: right;\">          3788.64</td></tr>\n",
       "<tr><td>_trainable_56074ffd</td><td>TERMINATED</td><td>192.168.1.136:64503</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">        60</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">0.00498844 </td><td>nb               </td><td style=\"text-align: right;\">          3648.34</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/impl/tuner_internal.py:144: RayDeprecationWarning: The `RunConfig` class should be imported from `ray.tune` when passing it to the Tuner. Please update your imports. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "  _log_deprecation_warning(\n",
      "2025-07-15 15:47:09,089\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-07-15 15:47:09,891\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
      "2025-07-15 15:47:09,893\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "\u001b[36m(_trainable pid=42999)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=42999)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=42999)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=42999)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=42999)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=42999)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=42999)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=42999)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=42999)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=42999)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=42999)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "\u001b[36m(_trainable pid=43842)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=43842)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=43842)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=43842)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=43842)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=43842)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=43842)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=43842)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=43842)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=43842)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=43974)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=43974)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=43974)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=43974)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=43974)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=43974)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=43974)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=43974)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=43974)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=43974)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=44127)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=44127)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=44127)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=44127)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=44127)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=44127)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=44127)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=44127)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=44127)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=44127)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=44454)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=44454)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=44454)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=44454)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=44454)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=44454)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=44454)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=44454)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=44454)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=44454)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=44803)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=44803)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=44803)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=44803)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=44803)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=44803)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=44803)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=44803)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=44803)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=44803)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=44933)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=44933)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=44933)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=44933)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=44933)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=44933)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=44933)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=44933)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=44933)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=44933)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=45079)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=45079)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=45079)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=45079)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=45079)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=45079)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=45079)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=45079)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=45079)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=45079)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=45215)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=45215)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=45215)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=45215)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=45215)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=45215)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=45215)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=45215)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=45215)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=45215)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=45355)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=45355)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=45355)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=45355)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=45355)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=45355)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=45355)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=45355)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=45355)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=45355)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=45481)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=45481)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=45481)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=45481)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=45481)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=45481)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=45481)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=45481)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=45481)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=45481)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=45635)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=45635)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=45635)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=45635)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=45635)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=45635)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=45635)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=45635)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=45635)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=45635)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=45833)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=45833)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=45833)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=45833)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=45833)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=45833)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=45833)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=45833)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=45833)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=45833)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=45957)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=45957)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=45957)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=45957)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=45957)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=45957)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=45957)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=45957)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=45957)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=45957)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=46114)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=46114)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=46114)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=46114)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=46114)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=46114)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=46114)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=46114)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=46114)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=46114)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=46243)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=46243)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=46243)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=46243)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=46243)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=46243)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=46243)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=46243)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=46243)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=46243)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=47033)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=47033)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=47033)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=47033)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=47033)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=47033)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=47033)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=47033)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=47033)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=47033)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=47201)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=47201)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=47201)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=47201)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=47201)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=47201)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=47201)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=47201)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=47201)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=47201)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=47329)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=47329)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=47329)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=47329)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=47329)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=47329)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=47329)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=47329)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=47329)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=47329)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=47480)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=47480)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=47480)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=47480)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=47480)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=47480)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=47480)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=47480)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=47480)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=47480)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=47614)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=47614)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=47614)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=47614)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=47614)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=47614)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=47614)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=47614)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=47614)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=47614)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=47614)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "\u001b[36m(_trainable pid=48678)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=48678)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=48678)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=48678)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=48678)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=48678)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=48678)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=48678)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=48678)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=48678)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=48952)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=48952)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=48952)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=48952)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=48952)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=48952)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=48952)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=48952)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=48952)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=48952)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=49317)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=49317)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=49317)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=49317)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=49317)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=49317)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=49317)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=49317)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=49317)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=49317)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=49586)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=49586)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=49586)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=49586)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=49586)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=49586)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=49586)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=49586)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=49586)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=49586)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=49870)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=49870)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=49870)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=49870)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=49870)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=49870)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=49870)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=49870)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=49870)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=49870)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=50071)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=50071)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=50071)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=50071)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=50071)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=50071)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=50071)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=50071)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=50071)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=50071)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=50071)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "\u001b[36m(_trainable pid=51360)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=51360)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=51360)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=51360)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=51360)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=51360)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=51360)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=51360)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=51360)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=51360)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=51643)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=51643)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=51643)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=51643)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=51643)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=51643)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=51643)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=51643)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=51643)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=51643)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=53157)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=53157)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=53157)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=53157)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=53157)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=53157)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=53157)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=53157)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=53157)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=53157)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=53495)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=53495)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=53495)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=53495)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=53495)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=53495)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=53495)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=53495)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=53495)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=53495)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=53942)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=53942)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=53942)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=53942)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=53942)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=53942)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=53942)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=53942)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=53942)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=53942)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=54625)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=54625)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=54625)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=54625)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=54625)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=54625)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=54625)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=54625)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=54625)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=54625)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=54913)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=54913)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=54913)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=54913)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=54913)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=54913)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=54913)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=54913)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=54913)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=54913)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=55094)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=55094)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=55094)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=55094)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=55094)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=55094)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=55094)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=55094)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=55094)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=55094)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=55352)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=55352)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=55352)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=55352)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=55352)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=55352)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=55352)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=55352)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=55352)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=55352)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=55352)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "\u001b[36m(_trainable pid=56119)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=56119)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=56119)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=56119)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=56119)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=56119)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=56119)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=56119)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=56119)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=56119)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=56290)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=56290)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=56290)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=56290)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=56290)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=56290)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=56290)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=56290)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=56290)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=56290)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=56719)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=56719)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=56719)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=56719)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=56719)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=56719)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=56719)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=56719)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=56719)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=56719)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=56887)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=56887)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=56887)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=56887)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=56887)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=56887)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=56887)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=56887)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=56887)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=56887)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=57173)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=57173)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=57173)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=57173)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=57173)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=57173)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=57173)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=57173)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=57173)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=57173)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=57329)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=57329)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=57329)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=57329)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=57329)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=57329)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=57329)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=57329)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=57329)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=57329)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=57475)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=57475)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=57475)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=57475)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=57475)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=57475)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=57475)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=57475)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=57475)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=57475)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=57661)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=57661)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=57661)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=57661)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=57661)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=57661)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=57661)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=57661)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=57661)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=57661)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=57792)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=57792)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=57792)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=57792)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=57792)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=57792)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=57792)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=57792)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=57792)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=57792)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=57792)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "\u001b[36m(_trainable pid=58562)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=58562)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=58562)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=58562)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=58562)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=58562)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=58562)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=58562)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=58562)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=58562)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=58702)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=58702)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=58702)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=58702)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=58702)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=58702)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=58702)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=58702)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=58702)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=58702)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=58864)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=58864)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=58864)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=58864)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=58864)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=58864)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=58864)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=58864)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=58864)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=58864)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=58864)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "\u001b[36m(_trainable pid=59410)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=59410)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=59410)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=59410)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=59410)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=59410)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=59410)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=59410)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=59410)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=59410)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=59620)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=59620)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=59620)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=59620)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=59620)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=59620)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=59620)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=59620)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=59620)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=59620)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=59785)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=59785)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=59785)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=59785)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=59785)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=59785)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=59785)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=59785)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=59785)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=59785)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=59978)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=59978)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=59978)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=59978)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=59978)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=59978)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=59978)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=59978)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=59978)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=59978)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=60206)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=60206)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=60206)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=60206)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=60206)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=60206)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=60206)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=60206)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=60206)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=60206)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=60346)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=60346)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=60346)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=60346)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=60346)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=60346)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=60346)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=60346)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=60346)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=60346)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=60346)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "\u001b[36m(_trainable pid=60937)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=60937)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=60937)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=60937)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=60937)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=60937)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=60937)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=60937)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=60937)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=60937)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61064)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=61064)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=61064)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=61064)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=61064)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=61064)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=61064)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=61064)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=61064)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61064)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61182)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=61182)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=61182)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=61182)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=61182)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=61182)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=61182)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=61182)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=61182)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61182)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61336)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=61336)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=61336)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=61336)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=61336)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=61336)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=61336)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=61336)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=61336)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61336)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61465)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=61465)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=61465)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=61465)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=61465)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=61465)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=61465)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=61465)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=61465)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61465)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61595)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=61595)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=61595)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=61595)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=61595)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=61595)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=61595)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=61595)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=61595)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61595)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61733)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=61733)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=61733)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=61733)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=61733)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=61733)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=61733)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=61733)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=61733)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61733)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61864)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=61864)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=61864)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=61864)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=61864)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=61864)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=61864)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=61864)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=61864)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61864)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61864)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "\u001b[36m(_trainable pid=62323)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=62323)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=62323)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=62323)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=62323)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=62323)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=62323)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=62323)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=62323)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=62323)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=62323)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "\u001b[36m(_trainable pid=62820)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=62820)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=62820)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=62820)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=62820)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=62820)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=62820)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=62820)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=62820)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=62820)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=62957)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=62957)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=62957)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=62957)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=62957)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=62957)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=62957)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=62957)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=62957)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=62957)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=63084)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=63084)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=63084)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=63084)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=63084)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=63084)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=63084)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=63084)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=63084)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=63084)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=63228)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=63228)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=63228)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=63228)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=63228)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=63228)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=63228)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=63228)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=63228)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=63228)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=63228)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "\u001b[36m(_trainable pid=63747)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=63747)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=63747)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=63747)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=63747)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=63747)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=63747)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=63747)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=63747)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=63747)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=63747)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "\u001b[36m(_trainable pid=64377)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=64377)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=64377)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=64377)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=64377)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=64377)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=64377)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=64377)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=64377)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=64377)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=64503)\u001b[0m An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "\u001b[36m(_trainable pid=64503)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=64503)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(_trainable pid=64503)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(_trainable pid=64503)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=64503)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=64503)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=64503)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=64503)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=64503)\u001b[0m /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "2025-07-15 16:45:33,315\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/sapien/Desktop/scdata/raw_adata/modified_data/scvi_log/autotune/2025-07-15_15-47-07_scvi' in 0.0237s.\n",
      "2025-07-15 16:45:33,339\tINFO tune.py:1041 -- Total run time: 3503.45 seconds (3503.37 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit(adata, metric=\"validation_loss\",\n",
    "                    resources = {'gpu': 1}, #have to specify gpu or might not use\n",
    "                    search_space = search_space,\n",
    "                   num_samples = 70,\n",
    "                   max_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f8bf668-2eb2-41b9-ac03-b7f3202a003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_vl = 10000\n",
    "best_i = 0\n",
    "for i, res in enumerate(results.results):\n",
    "    vl = res.metrics['validation_loss']\n",
    "\n",
    "    if vl < best_vl:\n",
    "        best_vl = vl\n",
    "        best_i = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c889c7c7-9982-43c1-a8cd-66fa757538ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d652592-0433-4be3-b5c1-64282ae36c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={'validation_loss': 3438.970947265625},\n",
       "  path='/home/sapien/Desktop/scdata/raw_adata/modified_data/scvi_log/autotune/2025-07-15_15-47-07_scvi/_trainable_434476a6_68_gene_likelihood=nb,lr=0.0065,n_hidden=256,n_latent=60,n_layers=1_2025-07-15_16-39-39',\n",
       "  filesystem='local',\n",
       "  checkpoint=None\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.results[best_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4763d28a-ee11-48bb-8ebe-d4ea7f80c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "scvi.model.SCVI.setup_anndata(adata,\n",
    "                              categorical_covariate_keys = ['samples'],\n",
    "                             continuous_covariate_keys=['pct_counts_mt', 'pct_counts_ribo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61aca865-1970-4d2a-83a5-8e078440c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = scvi.model.SCVI(adata, n_hidden = 256, n_latent = 60 , n_layers = 1 , gene_likelihood = 'nb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dc4cb62-91be-4eaf-bedf-c671f9484fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'lr': 0.0065}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "703c64fe-dc10-4521-b1e7-6cec9b00c6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200: 100%|█| 200/200 [27:02<00:00,  8.81s/it, v_num=1, train_loss_step=3.3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200: 100%|█| 200/200 [27:02<00:00,  8.11s/it, v_num=1, train_loss_step=3.3\n"
     ]
    }
   ],
   "source": [
    "model.train(max_epochs = 200, early_stopping = True, plan_kwargs = kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26459c61-c2a2-4965-b5dd-4453d441c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('the_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aca9ce18-2dec-4af3-b7cd-1a3c4bdf8e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.history['reconstruction_loss_validation']['reconstruction_loss_validation'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a672574-e47c-46a6-a80f-9a3e6357f5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.history['reconstruction_loss_train']['reconstruction_loss_train'], label='train')\n",
    "plt.plot(model.history['reconstruction_loss_validation']['reconstruction_loss_validation'], label='validation')\n",
    "\n",
    "plt.axhline(y, c = 'k')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c84654f6-7c0d-48fe-8ed2-9cfde8ab5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write_h5ad('temp.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34b31f9a-dbdb-4bce-b655-f9cb14555192",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obsm['X_scVI'] = model.get_latent_representation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0f4a9c6-dbbb-42e3-b889-3b0585401e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58015, 60)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obsm['X_scVI'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebeb69cf-31b5-45d9-9331-49653625f50a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypingError",
     "evalue": "Failed in nopython mode pipeline (step: nopython frontend)\nFailed in nopython mode pipeline (step: nopython frontend)\nUntyped global name 'print': Cannot determine Numba type of <class 'function'>\n\nFile \"../../../../anaconda3/envs/scCRC/lib/python3.9/site-packages/pynndescent/pynndescent_.py\", line 253:\ndef nn_descent_internal_low_memory_parallel(\n    <source elided>\n        if verbose:\n            print(\"\\t\", n + 1, \" / \", n_iters)\n            ^\n\nDuring: resolving callee type: type(CPUDispatcher(<function nn_descent_internal_low_memory_parallel at 0x7331007af4c0>))\nDuring: typing of call at /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/pynndescent/pynndescent_.py (359)\n\nDuring: resolving callee type: type(CPUDispatcher(<function nn_descent_internal_low_memory_parallel at 0x7331007af4c0>))\nDuring: typing of call at /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/pynndescent/pynndescent_.py (359)\n\n\nFile \"../../../../anaconda3/envs/scCRC/lib/python3.9/site-packages/pynndescent/pynndescent_.py\", line 359:\ndef nn_descent(\n    <source elided>\n    if low_memory:\n        nn_descent_internal_low_memory_parallel(\n        ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_scVI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/scCRC/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:191\u001b[0m, in \u001b[0;36mneighbors\u001b[0;34m(adata, n_neighbors, n_pcs, use_rep, knn, method, transformer, metric, metric_kwds, random_state, key_added, copy)\u001b[0m\n\u001b[1;32m    189\u001b[0m     adata\u001b[38;5;241m.\u001b[39m_init_as_actual(adata\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m    190\u001b[0m neighbors \u001b[38;5;241m=\u001b[39m Neighbors(adata)\n\u001b[0;32m--> 191\u001b[0m \u001b[43mneighbors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_neighbors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_pcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_pcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mknn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mknn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_added \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     key_added \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneighbors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/scCRC/lib/python3.9/site-packages/scanpy/neighbors/__init__.py:576\u001b[0m, in \u001b[0;36mNeighbors.compute_neighbors\u001b[0;34m(self, n_neighbors, n_pcs, use_rep, knn, method, transformer, metric, metric_kwds, random_state)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknn \u001b[38;5;241m=\u001b[39m knn\n\u001b[1;32m    575\u001b[0m X \u001b[38;5;241m=\u001b[39m _choose_representation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adata, use_rep\u001b[38;5;241m=\u001b[39muse_rep, n_pcs\u001b[38;5;241m=\u001b[39mn_pcs)\n\u001b[0;32m--> 576\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distances \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m knn_indices, knn_distances \u001b[38;5;241m=\u001b[39m _get_indices_distances_from_sparse_matrix(\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distances, n_neighbors\n\u001b[1;32m    579\u001b[0m )\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shortcut:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;66;03m# self._distances is a sparse matrix with a diag of 1, fix that\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/scCRC/lib/python3.9/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/scCRC/lib/python3.9/site-packages/pynndescent/pynndescent_.py:2256\u001b[0m, in \u001b[0;36mPyNNDescentTransformer.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   2236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[1;32m   2237\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit to graph_data, then transform it.\u001b[39;00m\n\u001b[1;32m   2238\u001b[0m \n\u001b[1;32m   2239\u001b[0m \u001b[38;5;124;03m    Fits transformer to X and y with optional parameters fit_params\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[38;5;124;03m        The diagonal is always explicit.\u001b[39;00m\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompress_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2257\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/anaconda3/envs/scCRC/lib/python3.9/site-packages/pynndescent/pynndescent_.py:2174\u001b[0m, in \u001b[0;36mPyNNDescentTransformer.fit\u001b[0;34m(self, X, compress_index)\u001b[0m\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;66;03m# Compatibility with sklearn, which doesn't consider\u001b[39;00m\n\u001b[1;32m   2171\u001b[0m \u001b[38;5;66;03m# a point its own neighbor for these purposes.\u001b[39;00m\n\u001b[1;32m   2172\u001b[0m effective_n_neighbors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_neighbors \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_ \u001b[38;5;241m=\u001b[39m \u001b[43mNNDescent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meffective_n_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_trees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mleaf_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleaf_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpruning_degree_multiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpruning_degree_multiplier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiversify_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiversify_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_search_trees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_search_trees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtree_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_termination_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompressed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompress_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_batch_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_batch_queries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2194\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/scCRC/lib/python3.9/site-packages/pynndescent/pynndescent_.py:923\u001b[0m, in \u001b[0;36mNNDescent.__init__\u001b[0;34m(self, data, metric, metric_kwds, n_neighbors, n_trees, leaf_size, pruning_degree_multiplier, diversify_prob, n_search_trees, tree_init, init_graph, init_dist, random_state, low_memory, max_candidates, max_rptree_depth, n_iters, delta, n_jobs, compressed, parallel_batch_queries, verbose)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    921\u001b[0m         \u001b[38;5;28mprint\u001b[39m(ts(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNN descent for\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(n_iters), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 923\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_neighbor_graph \u001b[38;5;241m=\u001b[39m \u001b[43mnn_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43meffective_max_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrp_tree_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_init_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleaf_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleaf_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_neighbor_graph[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    939\u001b[0m     warn(\n\u001b[1;32m    940\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to correctly find n_neighbors for some samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    941\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Results may be less than ideal. Try re-running with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m different parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/scCRC/lib/python3.9/site-packages/numba/core/dispatcher.py:423\u001b[0m, in \u001b[0;36m_DispatcherBase._compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    419\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThis error may have been caused \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby the following argument(s):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00margs_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    421\u001b[0m         e\u001b[38;5;241m.\u001b[39mpatch_message(msg)\n\u001b[0;32m--> 423\u001b[0m     \u001b[43merror_rewrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtyping\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mUnsupportedError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# Something unsupported is present in the user code, add help info\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     error_rewrite(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsupported_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/scCRC/lib/python3.9/site-packages/numba/core/dispatcher.py:364\u001b[0m, in \u001b[0;36m_DispatcherBase._compile_for_args.<locals>.error_rewrite\u001b[0;34m(e, issue_type)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypingError\u001b[0m: Failed in nopython mode pipeline (step: nopython frontend)\nFailed in nopython mode pipeline (step: nopython frontend)\nUntyped global name 'print': Cannot determine Numba type of <class 'function'>\n\nFile \"../../../../anaconda3/envs/scCRC/lib/python3.9/site-packages/pynndescent/pynndescent_.py\", line 253:\ndef nn_descent_internal_low_memory_parallel(\n    <source elided>\n        if verbose:\n            print(\"\\t\", n + 1, \" / \", n_iters)\n            ^\n\nDuring: resolving callee type: type(CPUDispatcher(<function nn_descent_internal_low_memory_parallel at 0x7331007af4c0>))\nDuring: typing of call at /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/pynndescent/pynndescent_.py (359)\n\nDuring: resolving callee type: type(CPUDispatcher(<function nn_descent_internal_low_memory_parallel at 0x7331007af4c0>))\nDuring: typing of call at /home/sapien/anaconda3/envs/scCRC/lib/python3.9/site-packages/pynndescent/pynndescent_.py (359)\n\n\nFile \"../../../../anaconda3/envs/scCRC/lib/python3.9/site-packages/pynndescent/pynndescent_.py\", line 359:\ndef nn_descent(\n    <source elided>\n    if low_memory:\n        nn_descent_internal_low_memory_parallel(\n        ^\n"
     ]
    }
   ],
   "source": [
    "sc.pp.neighbors(adata, use_rep='X_scVI', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e44a1c-5a98-4717-81f8-d8df1b341bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e8e18e-52af-4f9f-90bb-b7923a8f3dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d61f666-7684-483f-af92-e3e73e538c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
